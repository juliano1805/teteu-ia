import os
import sqlite3
import subprocess
import requests
from openai import OpenAI
from RestrictedPython import compile_restricted, safe_globals
import chardet
import glob
import cohere
from deep_translator import GoogleTranslator
from github import Github
from win10toast import ToastNotifier
import readline
import nbformat as nbf

# Substitua a configura√ß√£o da API key:
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
if not client.api_key:
    raise ValueError("API Key da OpenAI n√£o encontrada. Verifique a vari√°vel de ambiente 'OPENAI_API_KEY'.")

modelo_gpt = "gpt-3.5-turbo"

# üì¶ Banco de Dados SQLite
con = sqlite3.connect('teteu.db')
cur = con.cursor()

# Cria tabela se n√£o existir
cur.execute('''
    CREATE TABLE IF NOT EXISTS historico (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        comando TEXT,
        resposta TEXT
    )
''')
con.commit()

def obter_contexto(limite=5):
    cur.execute('SELECT comando, resposta FROM historico ORDER BY id DESC LIMIT ?', (limite,))
    registros = cur.fetchall()
    # Inverter para ordem cronol√≥gica
    return registros[::-1]

# Fun√ß√£o para conversar com o GPT (atualizada para openai>=1.0.0)
def perguntar_ao_gpt(prompt, contexto_limite=5):
    try:
        mensagens = []
        # Adiciona contexto das √∫ltimas intera√ß√µes
        contexto = obter_contexto(contexto_limite)
        for comando, resposta in contexto:
            mensagens.append({"role": "user", "content": comando})
            mensagens.append({"role": "assistant", "content": resposta})
        # Adiciona a nova pergunta com personalidade
        prompt_personalizado = (
            "Voc√™ √© o TETEU, um assistente de programa√ß√£o divertido, que responde de forma descontra√≠da e amig√°vel, "
            "usando g√≠rias brasileiras quando poss√≠vel. "
            "Seja claro, did√°tico e incentive o usu√°rio a aprender. "
            f"Pergunta do usu√°rio: {prompt}"
        )
        mensagens.append({"role": "user", "content": prompt_personalizado})

        resposta = client.chat.completions.create(
            model=modelo_gpt,
            messages=mensagens,
            temperature=0.7,
            max_tokens=1500
        )
        texto = resposta.choices[0].message.content.strip()
        return texto
    except Exception as e:
        if "insufficient_quota" in str(e):
            return "‚ö†Ô∏è Sua cota da OpenAI acabou. Verifique seu plano e billing em https://platform.openai.com/account/usage"
        return f"Erro ao acessar OpenAI: {e}"

# ‚öôÔ∏è Fun√ß√£o para executar c√≥digo Python de forma segura
def executar_codigo(codigo):
    try:
        byte_code = compile_restricted(codigo, '<string>', 'exec')
        exec(byte_code, safe_globals.copy())
    except Exception as e:
        print(f"Erro ao executar: {e}")

# üìú Mostrar hist√≥rico
def mostrar_historico(limite=10):
    cur.execute('SELECT id, comando, resposta FROM historico ORDER BY id DESC LIMIT ?', (limite,))
    registros = cur.fetchall()
    if registros:
        for r in registros[::-1]:
            print(f"\nüÜî {r[0]} ‚Äî Comando: {r[1]}\nResposta: {r[2]}")
    else:
        print("üì≠ Hist√≥rico vazio.")

# üîç Buscar no hist√≥rico
def buscar_no_historico(termo):
    cur.execute("SELECT id, comando, resposta FROM historico WHERE comando LIKE ? OR resposta LIKE ? ORDER BY id", 
                (f"%{termo}%", f"%{termo}%"))
    resultados = cur.fetchall()
    if resultados:
        for r in resultados:
            print(f"\nüÜî {r[0]} ‚Äî Comando: {r[1]}\nResposta: {r[2]}")
    else:
        print(f"üîç Nada encontrado para '{termo}'.")

def mostrar_ajuda():
    print("""
Comandos dispon√≠veis:
- sair: Encerra o programa
- exec <codigo>: Executa c√≥digo Python
- historico: Mostra as √∫ltimas intera√ß√µes
- buscar <termo>: Busca no hist√≥rico
- limpar_historico: Limpa todo o hist√≥rico
- exportar_historico: Exporta o hist√≥rico para um arquivo
- modelo <nome>: Troca o modelo GPT (ex: modelo gpt-4)
- explica <codigo>: Explica detalhadamente um c√≥digo Python
- resuma <texto>: Resume um texto longo
- erro <mensagem>: Explica um erro de Python e como resolver
- corrija <codigo>: Sugere melhorias e corrige um c√≥digo Python
- quiz: Recebe uma pergunta de m√∫ltipla escolha sobre programa√ß√£o
- desafio: Recebe um desafio de programa√ß√£o para praticar
- materiais: Sugere materiais gratuitos para aprender Python
- ajuda: Mostra esta mensagem de ajuda
""")

def limpar_historico():
    cur.execute('DELETE FROM historico')
    con.commit()
    print("üßπ Hist√≥rico limpo com sucesso!")

def exportar_historico():
    cur.execute('SELECT id, comando, resposta FROM historico ORDER BY id')
    registros = cur.fetchall()
    if registros:
        with open("historico_teteu.txt", "w", encoding="utf-8") as f:
            for r in registros:
                f.write(f"ID: {r[0]}\nComando: {r[1]}\nResposta: {r[2]}\n{'-'*40}\n")
        print("üì§ Hist√≥rico exportado para 'historico_teteu.txt'.")
    else:
        print("üì≠ Hist√≥rico vazio, nada para exportar.")

def exportar_para_notebook():
    cur.execute('SELECT comando, resposta FROM historico ORDER BY id')
    registros = cur.fetchall()
    nb = nbf.v4.new_notebook()
    cells = []
    for comando, resposta in registros:
        cells.append(nbf.v4.new_markdown_cell(f"**Comando:** `{comando}`\n\n**Resposta:**\n{resposta}"))
    nb['cells'] = cells
    with open("historico_teteu.ipynb", "w", encoding="utf-8") as f:
        nbf.write(nb, f)
    print("üìì Hist√≥rico exportado para 'historico_teteu.ipynb'.")

def explicar_codigo(codigo):
    prompt = f"Explique detalhadamente o que faz o seguinte c√≥digo Python:\n\n{codigo}"
    return perguntar_ao_gpt(prompt)

def resumir_texto(texto):
    prompt = f"Resuma o texto a seguir em poucas linhas, de forma clara e objetiva:\n\n{texto}"
    return perguntar_ao_gpt(prompt)

def explicar_erro(erro):
    prompt = f"Explique o seguinte erro de Python e como resolv√™-lo:\n\n{erro}"
    return perguntar_ao_gpt(prompt)

def corrigir_codigo(codigo):
    prompt = f"Revise o seguinte c√≥digo Python, aponte erros e sugira melhorias:\n\n{codigo}"
    return perguntar_ao_gpt(prompt)

def sugerir_materiais():
    prompt = "Sugira materiais gratuitos para aprender Python, como sites, v√≠deos e livros."
    return perguntar_ao_gpt(prompt)

def quiz_programacao():
    prompt = (
        "Crie uma pergunta de m√∫ltipla escolha sobre programa√ß√£o Python, "
        "com 4 alternativas e indique a correta no final."
    )
    return perguntar_ao_gpt(prompt)

def desafio_programacao():
    prompt = (
        "Me proponha um desafio simples de programa√ß√£o em Python para iniciantes, "
        "explique o que deve ser feito e mostre a solu√ß√£o ao final."
    )
    return perguntar_ao_gpt(prompt)

def explicar_biblioteca(nome):
    prompt = f"Explique para que serve a biblioteca Python '{nome}' e mostre um exemplo de uso."
    return perguntar_ao_gpt(prompt)

def buscar_stackoverflow(pergunta):
    url = "https://api.stackexchange.com/2.3/search/advanced"
    params = {
        "order": "desc",
        "sort": "relevance",
        "q": pergunta,
        "site": "stackoverflow",
        "accepted": True,
        "answers": 1
    }
    resp = requests.get(url, params=params)
    data = resp.json()
    if data["items"]:
        titulo = data["items"][0]["title"]
        link = data["items"][0]["link"]
        return f"Encontrei isso no Stack Overflow:\n{titulo}\n{link}"
    else:
        return "Nenhuma resposta encontrada no Stack Overflow."

import subprocess
import os

def analisar_codigo(codigo):
    with open("temp_code.py", "w", encoding="utf-8") as f:
        f.write(codigo)
    resultados = []
    try:
        # Flake8
        flake8 = subprocess.run(["flake8", "temp_code.py"], capture_output=True, text=True)
        if flake8.stdout:
            resultados.append(f"‚ö†Ô∏è Flake8 encontrou problemas:\n{flake8.stdout}")
        else:
            resultados.append("‚úÖ Flake8: Nenhum problema encontrado.")

        # Pylint
        pylint = subprocess.run(
            ["pylint", "temp_code.py", "--disable=all", "--enable=errors,warnings"],
            capture_output=True, text=True
        )
        if pylint.stdout and "Your code has been rated" not in pylint.stdout:
            resultados.append(f"‚ö†Ô∏è Pylint encontrou problemas:\n{pylint.stdout}")
        else:
            resultados.append("‚úÖ Pylint: Nenhum problema encontrado.")

        # Mypy (opcional, se usar type hints)
        mypy = subprocess.run(["mypy", "temp_code.py"], capture_output=True, text=True)
        if mypy.stdout and "Success" not in mypy.stdout:
            resultados.append(f"‚ö†Ô∏è Mypy encontrou problemas de tipos:\n{mypy.stdout}")
        else:
            resultados.append("‚úÖ Mypy: Tipos corretos ou n√£o utilizados.")

        # Black (sugest√£o de formata√ß√£o)
        black = subprocess.run(["black", "--check", "temp_code.py"], capture_output=True, text=True)
        if "would reformat" in black.stdout:
            resultados.append("üí° Black: O c√≥digo pode ser melhor formatado. Use o Black para padronizar.")
        else:
            resultados.append("‚úÖ Black: O c√≥digo j√° est√° bem formatado.")
    finally:
        if os.path.exists("temp_code.py"):
            os.remove("temp_code.py")
    return "\n\n".join(resultados)

def sugerir_projetos(nivel="iniciante"):
    prompt = f"Me sugira 3 ideias de projetos em Python para o n√≠vel {nivel}, com breve descri√ß√£o de cada."
    return perguntar_ao_gpt(prompt)

def revisar_com_gpt(codigo):
    prompt = (
        "Analise o seguinte c√≥digo Python, explique os principais erros encontrados, "
        "mostre exemplos de c√≥digo corrigido quando poss√≠vel e sugira melhorias de clareza, performance e boas pr√°ticas:\n\n"
        f"{codigo}"
    )
    return perguntar_ao_gpt(prompt)

# üß† Loop principal do TETEU
def teteu_loop():
    global modelo_gpt
    print("ü§ñ TETEU IA ‚Äî Assistente de C√≥digo")
    print("Comandos: 'sair', 'exec <codigo>', 'historico', 'buscar <termo>'")

    while True:
        comando = input(">>> ")

        if comando.lower() == 'sair':
            print("Tchau! üëã")
            break

        elif comando.lower().startswith('exec'):
            codigo = comando[5:]
            executar_codigo(codigo)
            print("C√≥digo executado.")
            continue

        elif comando.lower() == 'historico':
            texto_hist = mostrar_historico_interface()
            print(texto_hist)
            continue

        elif comando.lower().startswith('buscar'):
            termo = comando[7:].strip()
            if termo:
                buscar_no_historico(termo)
            else:
                print("‚ö†Ô∏è Informe um termo para buscar. Exemplo: buscar print")
            continue

        elif comando.lower() == 'ajuda':
            mostrar_ajuda()
            continue

        elif comando.lower() == 'limpar_historico':
            limpar_historico()
            print("üßπ Hist√≥rico limpo com sucesso!")
            continue

        elif comando.lower() == 'exportar_historico':
            exportar_historico()
            print("Hist√≥rico exportado para 'historico_teteu.txt'.")
            continue

        elif comando.lower() == 'exportar_para_notebook':
            exportar_para_notebook()
            print("Hist√≥rico exportado para 'historico_teteu.ipynb'.")
            continue

        elif comando.lower().startswith('modelo'):
            novo_modelo = comando[7:].strip()
            if novo_modelo:
                modelo_gpt = novo_modelo
                print(f"ü§ñ Modelo alterado para: {modelo_gpt}")
            else:
                print("‚ö†Ô∏è Informe o nome do modelo. Exemplo: modelo gpt-4")
            continue

        elif comando.lower().startswith('explica'):
            codigo = comando[8:]
            explicacao = perguntar_todas_ias(f"Explique detalhadamente o que faz o seguinte c√≥digo Python:\n\n{codigo}")
            print(f"\nTETEU ü§ñ {explicacao}")
            cur.execute('INSERT INTO historico (comando, resposta) VALUES (?, ?)', (comando, explicacao))
            con.commit()
            continue

        elif comando.lower().startswith('resuma'):
            texto = comando[7:]
            resumo = resumir_texto(texto)
            print(f"\nTETEU ü§ñ {resumo}")
            cur.execute('INSERT INTO historico (comando, resposta) VALUES (?, ?)', (comando, resumo))
            con.commit()
            continue

        elif comando.lower().startswith('erro'):
            erro = comando[5:]
            explicacao = explicar_erro(erro)
            print(f"\nTETEU ü§ñ {explicacao}")
            cur.execute('INSERT INTO historico (comando, resposta) VALUES (?, ?)', (comando, explicacao))
            con.commit()
            continue

        elif comando.lower() == 'quiz':
            quiz = quiz_programacao()
            print(f"\nTETEU ü§ñ {quiz}")
            cur.execute('INSERT INTO historico (comando, resposta) VALUES (?, ?)', (comando, quiz))
            con.commit()
            continue

        elif comando.lower() == 'desafio':
            desafio = desafio_programacao()
            print(f"\nTETEU ü§ñ {desafio}")
            cur.execute('INSERT INTO historico (comando, resposta) VALUES (?, ?)', (comando, desafio))
            con.commit()
            continue

        elif comando.lower().startswith('corrija'):
            codigo = comando[7:]
            correcao = corrigir_codigo(codigo)
            print(f"\nTETEU ü§ñ {correcao}")
            cur.execute('INSERT INTO historico (comando, resposta) VALUES (?, ?)', (comando, correcao))
            con.commit()
            continue

        elif comando.lower() == 'materiais':
            materiais = sugerir_materiais()
            print(f"\nTETEU ü§ñ {materiais}")
            cur.execute('INSERT INTO historico (comando, resposta) VALUES (?, ?)', (comando, materiais))
            con.commit()
            continue

        elif comando.lower().startswith('stackoverflow'):
            pergunta = comando[13:].strip()
            resposta = buscar_stackoverflow(pergunta)
            print(f"\nTETEU ü§ñ {resposta}")
            cur.execute('INSERT INTO historico (comando, resposta) VALUES (?, ?)', (comando, resposta))
            con.commit()
            continue

        elif comando.lower().startswith('biblioteca'):
            nome = comando[11:].strip()
            explicacao = explicar_biblioteca(nome)
            print(f"\nTETEU ü§ñ {explicacao}")
            cur.execute('INSERT INTO historico (comando, resposta) VALUES (?, ?)', (comando, explicacao))
            con.commit()
            continue

        elif comando.lower().startswith('analisar'):
            codigo = comando[8:].strip()
            analise = analisar_codigo(codigo)
            print(f"\nTETEU ü§ñ {analise}")
            try:
                revisao = revisar_com_gpt(codigo)
                print(f"\nTETEU ü§ñ Sugest√£o GPT:\n{revisao}")
                resposta_final = analise + "\n\nSugest√£o GPT:\n" + revisao
            except Exception as e:
                print("\nTETEU ü§ñ N√£o foi poss√≠vel acessar o GPT para revis√£o. Mostrando apenas an√°lise autom√°tica.")
                resposta_final = analise
            cur.execute('INSERT INTO historico (comando, resposta) VALUES (?, ?)', (comando, resposta_final))
            con.commit()
            continue

        elif comando.lower().startswith('projetos'):
            nivel = comando[8:].strip() or "iniciante"
            sugestoes = sugerir_projetos(nivel)
            print(f"\nTETEU ü§ñ {sugestoes}")
            cur.execute('INSERT INTO historico (comando, resposta) VALUES (?, ?)', (comando, sugestoes))
            con.commit()
            continue

        else:
            print("ü§ñ TETEU: N√£o entendi esse comando. Digite 'ajuda' para ver as op√ß√µes.")

def limpar_arquivos_temporarios(padrao="temp_code*.py"):
    for arquivo in glob.glob(padrao):
        try:
            os.remove(arquivo)
        except Exception as e:
            print(f"N√£o foi poss√≠vel remover {arquivo}: {e}")

# üöÄ Rodar
if __name__ == "__main__":
    try:
        teteu_loop()
    finally:
        limpar_arquivos_temporarios()
        con.close()

def perguntar_ao_cohere(prompt):
    try:
        co = cohere.Client(os.getenv("COHERE_API_KEY"))  # Ou coloque sua chave direto aqui
        resposta = co.generate(
            model='command',  # ou 'command-light'
            prompt=prompt,
            max_tokens=300
        )
        return resposta.generations[0].text.strip()
    except Exception as e:
        return f"Erro Cohere: {e}"

def perguntar_ao_huggingface(prompt):
    try:
        API_URL = "https://api-inference.huggingface.co/models/bigscience/bloomz-560m"  # Voc√™ pode trocar por outro modelo!
        headers = {"Authorization": f"Bearer {os.getenv('HF_API_KEY')}"}
        payload = {"inputs": prompt}
        response = requests.post(API_URL, headers=headers, json=payload, timeout=30)
        resposta = response.json()
        # Alguns modelos retornam uma lista, outros um dicion√°rio
        if isinstance(resposta, list) and resposta and "generated_text" in resposta[0]:
            return resposta[0]["generated_text"].strip()
        elif isinstance(resposta, dict) and "error" in resposta:
            return f"Erro Hugging Face: {resposta['error']}"
        else:
            return str(resposta)
    except Exception as e:
        return f"Erro Hugging Face: {e}"

def perguntar_todas_ias(prompt, contexto_limite=5):
    respostas = []

    # OpenAI GPT
    try:
        resposta_gpt = perguntar_ao_gpt(prompt, contexto_limite)
        if resposta_gpt and not resposta_gpt.startswith("‚ö†Ô∏è Sua cota da OpenAI acabou"):
            respostas.append("ü§ñ GPT:\n" + resposta_gpt)
    except Exception:
        pass

    # Cohere
    try:
        prompt_en = traduzir_para_ingles(prompt)
        resposta_cohere = perguntar_ao_cohere(prompt_en)
        if resposta_cohere and not "quota" in str(resposta_cohere).lower():
            respostas.append("üü£ Cohere:\n" + resposta_cohere)
    except Exception:
        pass

    # Hugging Face
    try:
        prompt_en = traduzir_para_ingles(prompt)
        resposta_hf = perguntar_ao_huggingface(prompt_en)
        if resposta_hf and not "quota" in str(resposta_hf).lower():
            respostas.append("ü§ó Hugging Face:\n" + resposta_hf)
    except Exception:
        pass

    if respostas:
        return "\n\n".join(respostas)
    else:
        return "Nenhuma IA p√¥de responder no momento (todas atingiram o limite ou houve erro)."

def traduzir_para_ingles(texto):
    try:
        return GoogleTranslator(source='pt', target='en').translate(texto)
    except Exception:
        return texto

def baixar_repositorio_github(url):
    try:
        g = Github()
        repo_name = url.split("github.com/")[1]
        repo = g.get_repo(repo_name)
        arquivos = repo.get_contents("")
        for arquivo in arquivos:
            print(arquivo.name)
    except Exception as e:
        print(f"Erro ao baixar reposit√≥rio: {e}")

def notificar(texto):
    toaster = ToastNotifier()
    toaster.show_toast("TETEU IA", texto, duration=5)

def mostrar_historico_interface(limite=10):
    cur.execute('SELECT id, comando, resposta FROM historico ORDER BY id DESC LIMIT ?', (limite,))
    registros = cur.fetchall()
    if registros:
        texto = ""
        for r in registros[::-1]:
            texto += f"\nüÜî {r[0]} ‚Äî Comando: {r[1]}\nResposta: {r[2]}\n"
        return texto
    else:
        return "üì≠ Hist√≥rico vazio."
